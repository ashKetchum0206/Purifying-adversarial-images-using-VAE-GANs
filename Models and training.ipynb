{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":51404,"sourceType":"modelInstanceVersion","modelInstanceId":43272},{"sourceId":51412,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":43279},{"sourceId":51699,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":43489},{"sourceId":53178,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":44626}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision","metadata":{"id":"XqUJotsbIIoy","outputId":"ff786056-fcb5-408d-b6c3-802a58549b89","execution":{"iopub.status.busy":"2024-05-21T14:59:30.874714Z","iopub.execute_input":"2024-05-21T14:59:30.875030Z","iopub.status.idle":"2024-05-21T14:59:43.823940Z","shell.execute_reply.started":"2024-05-21T14:59:30.875004Z","shell.execute_reply":"2024-05-21T14:59:43.822726Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:43.825783Z","iopub.execute_input":"2024-05-21T14:59:43.826070Z","iopub.status.idle":"2024-05-21T14:59:48.903639Z","shell.execute_reply.started":"2024-05-21T14:59:43.826043Z","shell.execute_reply":"2024-05-21T14:59:48.902875Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define the transformation to include normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.130,), ( 0.308,))  # Normalize with mean=0.5 and std=0.5\n])\n\n# Data loading\ntrain_dataset = datasets.FashionMNIST(root = './data', train=True, download=True, transform=transform)\ndata_loader = DataLoader(train_dataset, batch_size = 128, shuffle=True)","metadata":{"id":"GeCHvUJIfujm","execution":{"iopub.status.busy":"2024-05-21T14:59:48.904765Z","iopub.execute_input":"2024-05-21T14:59:48.905282Z","iopub.status.idle":"2024-05-21T14:59:49.000907Z","shell.execute_reply.started":"2024-05-21T14:59:48.905257Z","shell.execute_reply":"2024-05-21T14:59:49.000166Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def imshow(img):\n    \"\"\" Function to show an image \"\"\"\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.detach().cpu().numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')  # Hide the axes\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.003561Z","iopub.execute_input":"2024-05-21T14:59:49.003865Z","iopub.status.idle":"2024-05-21T14:59:49.009201Z","shell.execute_reply.started":"2024-05-21T14:59:49.003840Z","shell.execute_reply":"2024-05-21T14:59:49.008186Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder\nclass Encoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.mu = nn.Linear(1000, 128)\n        self.sigma = nn.Linear(1000, 128)\n\n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        mu = self.mu(x)\n        log_var = self.sigma(x)\n        return mu, log_var\n\nclass Decoder(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        self.layer3 = nn.Linear(128, 1000)\n        self.layer4 = nn.Linear(1000, 256 * 14 * 14)\n        self.deconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(128)\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(64)\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n\n    def forward(self, z):\n\n        z = F.relu(self.layer3(z))\n        z = F.relu(self.layer4(z))\n        z = z.view(-1, 256, 14, 14)  # Reshape to match the deconv input size\n        z = F.relu(self.bn5(self.upsample1(self.deconv1(z))))\n        z = F.relu(self.bn6(self.deconv2(z)))\n        z = torch.sigmoid((self.deconv3(z)))  # Added another deconv layer and upsampling\n        return z\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        \n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        x = F.relu(self.bn5(self.pen_final(x)))\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features\n\n# VAE_GAN class modification to include discriminator features in the loss function\nclass VAE_GAN(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real = self.discriminator(x)\n        disc_recon, features_recon = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior,_= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior","metadata":{"id":"iizGZeY9IQd6","execution":{"iopub.status.busy":"2024-05-21T14:59:49.010425Z","iopub.execute_input":"2024-05-21T14:59:49.010709Z","iopub.status.idle":"2024-05-21T14:59:49.039978Z","shell.execute_reply.started":"2024-05-21T14:59:49.010686Z","shell.execute_reply":"2024-05-21T14:59:49.039082Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Discriminator_multi(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.disc_pen = nn.Linear(1000, 10)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n        self.bn6 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        self.final_dis = nn.Linear(10, 10)\n        \n#     def forward(self, x):\n\n#         x = F.relu(self.bn1(self.conv1(x)))\n#         x = F.relu(self.bn2(self.conv2(x)))\n#         x = F.relu(self.bn3(self.conv3(x)))\n#         x = self.maxpool3(x)\n#         x = x.view(-1, 256 * 14 * 14)\n#         x = F.relu(self.bn4(self.linear_layer(x)))\n#         y = F.relu(self.bn6(self.disc_pen(x)))\n#         x = F.relu(self.bn5(self.pen_final(x)))\n#         y = F.softmax(self.final_dis(y), dim = 1)\n#         features = torch.clone(x.detach())\n#         return F.sigmoid(self.final_gen(x)), features, y\n    \n    def forward(self, x):\n\n        x = F.relu((self.conv1(x)))\n        x = F.relu((self.conv2(x)))\n        x = F.relu((self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.relu((self.linear_layer(x)))\n        y = F.relu((self.disc_pen(x)))\n        x = F.relu((self.pen_final(x)))\n        y = F.softmax(self.final_dis(y), dim = 1)\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features, y","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.041106Z","iopub.execute_input":"2024-05-21T14:59:49.041369Z","iopub.status.idle":"2024-05-21T14:59:49.054366Z","shell.execute_reply.started":"2024-05-21T14:59:49.041344Z","shell.execute_reply":"2024-05-21T14:59:49.053509Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer_dis, optimizer_enc, optimizer_dec):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_dis': optimizer_dis.state_dict(),\n        'optimizer_enc': optimizer_enc.state_dict(),\n        'optimizer_dec': optimizer_dec.state_dict()\n    }\n    \n    torch.save(checkpoint, \"epoch{} checkpoint\".format(epoch))\n\n# Define a function to load a checkpoint\ndef load_checkpoint(model, optimizer_dis, optimizer_enc, optimizer_dec, filename):\n    \n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer_dis.load_state_dict(checkpoint['optimizer_dis'])\n    optimizer_enc.load_state_dict(checkpoint['optimizer_enc'])\n    optimizer_dec.load_state_dict(checkpoint['optimizer_dec'])\n    epoch = checkpoint['epoch']\n    return epoch","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.055285Z","iopub.execute_input":"2024-05-21T14:59:49.055542Z","iopub.status.idle":"2024-05-21T14:59:49.066107Z","shell.execute_reply.started":"2024-05-21T14:59:49.055510Z","shell.execute_reply":"2024-05-21T14:59:49.065354Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train(model, data_loader, optimizer_enc, optimizer_dec, optimizer_dis, epochs=5):\n    \n    try:\n        device = next(model.parameters()).device  # Get the device from the model's parameters\n        torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n        for p in model.parameters():\n            p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n        for epoch in range(epochs):\n            for batch_idx, (imgs, _) in enumerate(data_loader):\n                imgs = imgs.to(device)\n\n                # Zero the parameter gradients\n                optimizer_enc.zero_grad()\n                optimizer_dec.zero_grad()\n                optimizer_dis.zero_grad()\n\n                # Forward pass through VAE/GAN\n    #             print(model(imgs))\n                features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior = model(imgs)\n\n                mse_loss = nn.MSELoss(reduction = 'sum')\n                # Calculate losses\n                # Reconstruction loss and KL divergence for VAE\n                l_dis_like = mse_loss(features_real, features_recon)\n                l_prior = -5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    #             print(disc_real)\n                l_gan = torch.sum(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1 - disc_prior))\n                \n    #             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n    #             enc_loss = l_dis_like + l_prior \n    #             dec_loss = l_dis_like - l_gan\n                disc_loss = -l_gan\n\n    #             print(l_gan.item())\n                disc_loss.backward()\n    #             enc_loss.backward(retain_graph = True)\n    #             dec_loss.backward()\n\n    #             optimizer_dis.step()\n    #             optimizer_enc.step()\n\n\n    #             print(max_norm)\n    # Calculate the total norm of all gradients\n#                 total_norm = 0\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         total_norm += torch.norm(param.grad).item() ** 2\n#                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n#                 # Clip gradients if the total norm exceeds a threshold\n#                 max_norm = 100  # Adjust as needed\n#                 if total_norm > max_norm:\n#                     for name, param in model.discriminator.named_parameters():\n#                         if param.grad is not None:\n#                             param.grad *= max_norm / total_norm\n\n                optimizer_dis.step()\n\n                if(batch_idx % 50) == 0:             \n                    total_norm = 0\n                    for name, param in model.discriminator.named_parameters():\n                        if param.grad is not None:\n                            total_norm += torch.norm(param.grad).item() ** 2\n                    total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n                    print(total_norm)\n\n                if batch_idx % 1000 == 0:\n\n                    print(\"epoch:{}\".format(epoch))\n                    print(\"disc_loss:\", disc_loss.item())\n                    print('Original Images')\n                    imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                    # Pass images through the model to get the reconstructions\n                    with torch.no_grad():\n\n                        imgs = imgs.to(device)\n                        mu, log_var = model.encoder(imgs)\n                        noise = torch.rand_like(log_var)\n                        z = mu + noise * torch.exp(0.5 * log_var)\n                        recon_images = model.decoder(z)\n                        recon_images = recon_images.cpu()\n\n                    print('Reconstructed Images')\n                    imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n            if epoch%10 == 0:\n                torch.save(model.discriminator.state_dict(), 'disc.pt')\n    \n    except:\n        return","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.067294Z","iopub.execute_input":"2024-05-21T14:59:49.067685Z","iopub.status.idle":"2024-05-21T14:59:49.083518Z","shell.execute_reply.started":"2024-05-21T14:59:49.067654Z","shell.execute_reply":"2024-05-21T14:59:49.082741Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VAE_GAN_multi(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN_multi, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real, class_real = self.discriminator(x)\n        disc_recon, features_recon, _ = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior, _, _= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.084624Z","iopub.execute_input":"2024-05-21T14:59:49.084934Z","iopub.status.idle":"2024-05-21T14:59:49.095419Z","shell.execute_reply.started":"2024-05-21T14:59:49.084905Z","shell.execute_reply":"2024-05-21T14:59:49.094714Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train_multi(model, data_loader, optimizer_dis, epochs=5):\n    \n    try:\n        device = next(model.parameters()).device  # Get the device from the model's parameters\n        torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n    #         for p in model.parameters():\n    #             p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n        for epoch in range(epochs):\n            for batch_idx, (imgs, labels) in enumerate(data_loader):\n                imgs = imgs.to(device)\n                labels_one_hot = F.one_hot(labels, num_classes=10).float().to(device)\n\n                # Zero the parameter gradients\n                optimizer_dis.zero_grad()\n\n                # Forward pass through VAE/GAN\n    #             print(model(imgs))\n                features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = model(imgs)\n\n#                 mse_loss = nn.MSELoss(reduction = 'sum')\n                cross_entropy_loss = nn.CrossEntropyLoss()\n                # Calculate losses\n                # Reconstruction loss and KL divergence for VAE\n#                 l_dis_like = mse_loss(features_real, features_recon)\n#                 l_prior = -5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    #             print(disc_real)\n                l_gan = torch.sum(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1 - disc_prior))\n                l_class = cross_entropy_loss(class_real, labels_one_hot)\n\n    #             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n    #             enc_loss = l_dis_like + l_prior \n    #             dec_loss = l_dis_like - l_gan\n                disc_loss = -l_gan + l_class \n\n#                 print(l_gan.item())\n#                 print('disc_real:', disc_real)\n#                 print('disc_recon:',  disc_recon)\n#                 print('disc_prior:', disc_prior)\n#                 print('true_class_prob:', torch.exp(-l_class))\n#                 print('\\n')\n                disc_loss.backward()\n\n    #             enc_loss.backward(retain_graph = True)\n    #             dec_loss.backward()\n\n    #             optimizer_dis.step()\n    #             optimizer_enc.step()\n\n\n    #             print(max_norm)\n    # Calculate the total norm of all gradients\n    #                 total_norm = 0\n    #                 for name, param in model.discriminator.named_parameters():\n    #                     if param.grad is not None:\n    #                         total_norm += torch.norm(param.grad).item() ** 2\n    #                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n    #                 # Clip gradients if the total norm exceeds a threshold\n    #                 max_norm = 100  # Adjust as needed\n    #                 if total_norm > max_norm:\n    #                     for name, param in model.discriminator.named_parameters():\n    #                         if param.grad is not None:\n    #                             param.grad *= max_norm / total_norm\n\n                optimizer_dis.step()\n                if batch_idx % 1000 == 0:\n                \n                    print(\"epoch:{}\".format(epoch))\n                    print(\"disc_loss:\", disc_loss.item())\n                    print('Original Images')\n                    imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                    # Pass images through the model to get the reconstructions\n                    with torch.no_grad():\n\n                        imgs = imgs.to(device)\n                        mu, log_var = model.encoder(imgs)\n                        noise = torch.rand_like(log_var)\n                        z = mu + noise * torch.exp(0.5 * log_var)\n                        recon_images = model.decoder(z)\n                        recon_images = recon_images.cpu()\n\n                    print('Reconstructed Images')\n                    imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n            if epoch%10 == 0:\n                torch.save(model.discriminator.state_dict(), 'disc_multi.pt')\n                \n    except:\n        return","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:49.099027Z","iopub.execute_input":"2024-05-21T14:59:49.099294Z","iopub.status.idle":"2024-05-21T14:59:49.112679Z","shell.execute_reply.started":"2024-05-21T14:59:49.099273Z","shell.execute_reply":"2024-05-21T14:59:49.111956Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Initialize the models and optimizers\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = Encoder().to(device)\ndecoder = Decoder().to(device)\n# discriminator = Discriminator().to(device)","metadata":{"id":"j9P7UnA7z7l9","outputId":"42310e36-6714-4efe-8671-5a184f0a47d0","execution":{"iopub.status.busy":"2024-05-21T14:59:49.113668Z","iopub.execute_input":"2024-05-21T14:59:49.114177Z","iopub.status.idle":"2024-05-21T14:59:50.367814Z","shell.execute_reply.started":"2024-05-21T14:59:49.114154Z","shell.execute_reply":"2024-05-21T14:59:50.366789Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"encoder.load_state_dict(torch.load('/kaggle/input/encoder_normalized/pytorch/v1/1/encoder.pt'))\ndecoder.load_state_dict(torch.load('/kaggle/input/decoder_normalized/pytorch/v1/1/decoder.pt'))\n# discriminator.load_state_dict(torch.load('/kaggle/input/discriminator/pytorch/v1/1/disc.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:50.369247Z","iopub.execute_input":"2024-05-21T14:59:50.369536Z","iopub.status.idle":"2024-05-21T14:59:53.596917Z","shell.execute_reply.started":"2024-05-21T14:59:50.369513Z","shell.execute_reply":"2024-05-21T14:59:53.596042Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# vae_gan = VAE_GAN(encoder, decoder, discriminator).to(device)\noptimizer_enc = Adam(encoder.parameters(), lr=3e-5)\noptimizer_dec = Adam(decoder.parameters(), lr=3e-5)\n# optimizer_dis = Adam(discriminator.parameters(), lr=6e-5)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.597968Z","iopub.execute_input":"2024-05-21T14:59:53.598237Z","iopub.status.idle":"2024-05-21T14:59:53.603397Z","shell.execute_reply.started":"2024-05-21T14:59:53.598214Z","shell.execute_reply":"2024-05-21T14:59:53.602485Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train(vae_gan, data_loader, optimizer_enc, optimizer_dec, optimizer_dis, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.604484Z","iopub.execute_input":"2024-05-21T14:59:53.604819Z","iopub.status.idle":"2024-05-21T14:59:53.613151Z","shell.execute_reply.started":"2024-05-21T14:59:53.604797Z","shell.execute_reply":"2024-05-21T14:59:53.612363Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# torch.save(vae_gan.discriminator.state_dict(), 'disc.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.614170Z","iopub.execute_input":"2024-05-21T14:59:53.614438Z","iopub.status.idle":"2024-05-21T14:59:53.624190Z","shell.execute_reply.started":"2024-05-21T14:59:53.614417Z","shell.execute_reply":"2024-05-21T14:59:53.623468Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink \n# %cd /kaggle/working \n# FileLink('disc.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.625195Z","iopub.execute_input":"2024-05-21T14:59:53.625527Z","iopub.status.idle":"2024-05-21T14:59:53.631476Z","shell.execute_reply.started":"2024-05-21T14:59:53.625499Z","shell.execute_reply":"2024-05-21T14:59:53.630743Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# vae_gan.to('cpu')\n# vae_gan.eval()\n# out, _ = vae_gan.discriminator(train_dataset[8][0].unsqueeze(0))\n# out","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.632359Z","iopub.execute_input":"2024-05-21T14:59:53.632661Z","iopub.status.idle":"2024-05-21T14:59:53.639757Z","shell.execute_reply.started":"2024-05-21T14:59:53.632639Z","shell.execute_reply":"2024-05-21T14:59:53.638935Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# vals = []\n# vae_gan.to('cpu')\n# vae_gan.eval()\n# for el in tqdm(train_dataset):\n#     out, _ = vae_gan.discriminator(el[0].unsqueeze(0))\n#     vals.append(out.item())    ","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.640815Z","iopub.execute_input":"2024-05-21T14:59:53.641068Z","iopub.status.idle":"2024-05-21T14:59:53.647327Z","shell.execute_reply.started":"2024-05-21T14:59:53.641043Z","shell.execute_reply":"2024-05-21T14:59:53.646498Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# vals = np.array(vals)\n# np.mean(vals), np.std(vals)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.648379Z","iopub.execute_input":"2024-05-21T14:59:53.648664Z","iopub.status.idle":"2024-05-21T14:59:53.654269Z","shell.execute_reply.started":"2024-05-21T14:59:53.648642Z","shell.execute_reply":"2024-05-21T14:59:53.653349Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# encoder = Encoder().to(device)\n# decoder = Decoder().to(device)\n# optimizer_enc = Adam(encoder.parameters(), lr=3e-5)\n# optimizer_dec = Adam(decoder.parameters(), lr=3e-5)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.655288Z","iopub.execute_input":"2024-05-21T14:59:53.655626Z","iopub.status.idle":"2024-05-21T14:59:53.662003Z","shell.execute_reply.started":"2024-05-21T14:59:53.655597Z","shell.execute_reply":"2024-05-21T14:59:53.661235Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# epochs = 30\n# for epoch in range(epochs):\n    \n#     total_loss = 0\n#     count = 0\n#     for img, _ in data_loader:\n        \n#         optimizer_enc.zero_grad()\n#         optimizer_dec.zero_grad()\n        \n#         img = img.to(device)\n#         mu, log_var = encoder(img)\n#         noise = torch.rand_like(log_var)\n#         z = mu + noise * torch.exp(0.5 * log_var)\n#         recons = decoder(z)\n        \n#         criterion = nn.MSELoss(reduction = 'sum')\n#         loss = criterion(img, recons) + -5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        \n#         loss.backward()\n#         optimizer_enc.step()\n#         optimizer_dec.step()\n#         total_loss += loss\n#         count +=1\n        \n#     print('Original Images')\n#     imshow(torchvision.utils.make_grid(img[:10]))\n\n#     # Pass images through the model to get the reconstructions\n#     with torch.no_grad():\n\n#         img = img.to(device)\n#         mu, log_var = encoder(img)\n#         noise = torch.rand_like(log_var)\n#         z = mu + noise * torch.exp(0.5 * log_var)\n#         recon_images = decoder(z)\n#         recon_images = recon_images.cpu()\n\n#     print('Reconstructed Images')\n#     imshow(torchvision.utils.make_grid(recon_images[:10]))\n#     print(\"The epoch is {} and the loss is {}\".format(epoch, total_loss/count))","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.662905Z","iopub.execute_input":"2024-05-21T14:59:53.663168Z","iopub.status.idle":"2024-05-21T14:59:53.670397Z","shell.execute_reply.started":"2024-05-21T14:59:53.663146Z","shell.execute_reply":"2024-05-21T14:59:53.669622Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# torch.save(encoder.state_dict(), 'encoder.pt')\n# torch.save(decoder.state_dict(), 'decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.671331Z","iopub.execute_input":"2024-05-21T14:59:53.671615Z","iopub.status.idle":"2024-05-21T14:59:53.680307Z","shell.execute_reply.started":"2024-05-21T14:59:53.671585Z","shell.execute_reply":"2024-05-21T14:59:53.679622Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# %cd /kaggle/working\n# FileLink('encoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.681508Z","iopub.execute_input":"2024-05-21T14:59:53.681776Z","iopub.status.idle":"2024-05-21T14:59:53.688826Z","shell.execute_reply.started":"2024-05-21T14:59:53.681755Z","shell.execute_reply":"2024-05-21T14:59:53.687881Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# %cd /kaggle/working\n# FileLink('decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T14:59:53.690142Z","iopub.execute_input":"2024-05-21T14:59:53.690471Z","iopub.status.idle":"2024-05-21T14:59:53.695604Z","shell.execute_reply.started":"2024-05-21T14:59:53.690443Z","shell.execute_reply":"2024-05-21T14:59:53.694752Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"discriminator_multi = Discriminator_multi().to(device)\nvae_gan_multi = VAE_GAN_multi(encoder, decoder, discriminator_multi).to(device)\noptimizer_dis = Adam(discriminator_multi.parameters(), lr = 6e-5)","metadata":{"id":"iHqyUNNhY_CX","execution":{"iopub.status.busy":"2024-05-21T14:59:53.696632Z","iopub.execute_input":"2024-05-21T14:59:53.696881Z","iopub.status.idle":"2024-05-21T14:59:54.239954Z","shell.execute_reply.started":"2024-05-21T14:59:53.696860Z","shell.execute_reply":"2024-05-21T14:59:54.239191Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# discriminator_multi.load_state_dict(torch.load('/kaggle/input/discriminator_multi_normalized/pytorch/v1/1/disc_multi (1).pt'))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T17:41:00.615141Z","iopub.execute_input":"2024-05-20T17:41:00.615818Z","iopub.status.idle":"2024-05-20T17:41:02.094814Z","shell.execute_reply.started":"2024-05-20T17:41:00.615780Z","shell.execute_reply":"2024-05-20T17:41:02.093842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.encoder.eval()\nvae_gan_multi.decoder.eval()\n# vae_gan_multi.discriminator.eval()\ntrain_multi(vae_gan_multi, data_loader, optimizer_dis, epochs = 60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vae_gan_multi.discriminator.state_dict(), 'disc_multi.pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:26:12.937424Z","iopub.execute_input":"2024-05-17T18:26:12.938342Z","iopub.status.idle":"2024-05-17T18:26:13.326393Z","shell.execute_reply.started":"2024-05-17T18:26:12.938308Z","shell.execute_reply":"2024-05-17T18:26:13.325580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vals = []\nvae_gan_multi.to('cpu')\nvae_gan_multi.eval()\nfor i in tqdm(range(len(train_dataset))):\n    out, _, _ = vae_gan_multi.discriminator(train_dataset[i][0].unsqueeze(0))\n    vals.append(out.item())   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.eval()\n# vae_gan_multi.discriminator.train()\nvae_gan_multi.to('cpu')\nfor imgs, labels in data_loader:\n    features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = vae_gan_multi(imgs)\n    print(disc_real, disc_recon, disc_prior)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vals = np.array(vals)\nnp.mean(vals), np.std(vals)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:26:26.499379Z","iopub.execute_input":"2024-05-17T18:26:26.499692Z","iopub.status.idle":"2024-05-17T18:26:26.506360Z","shell.execute_reply.started":"2024-05-17T18:26:26.499666Z","shell.execute_reply":"2024-05-17T18:26:26.505474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}