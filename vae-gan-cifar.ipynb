{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2359346,"sourceType":"datasetVersion","datasetId":1424838}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision","metadata":{"id":"XqUJotsbIIoy","outputId":"ff786056-fcb5-408d-b6c3-802a58549b89","execution":{"iopub.status.busy":"2024-08-29T17:54:44.839259Z","iopub.execute_input":"2024-08-29T17:54:44.839966Z","iopub.status.idle":"2024-08-29T17:54:58.437570Z","shell.execute_reply.started":"2024-08-29T17:54:44.839910Z","shell.execute_reply":"2024-08-29T17:54:58.436349Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd ","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:54:58.439879Z","iopub.execute_input":"2024-08-29T17:54:58.440284Z","iopub.status.idle":"2024-08-29T17:54:58.446611Z","shell.execute_reply.started":"2024-08-29T17:54:58.440241Z","shell.execute_reply":"2024-08-29T17:54:58.445694Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:54:58.447900Z","iopub.execute_input":"2024-08-29T17:54:58.448313Z","iopub.status.idle":"2024-08-29T17:54:58.462246Z","shell.execute_reply.started":"2024-08-29T17:54:58.448269Z","shell.execute_reply":"2024-08-29T17:54:58.461360Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"metadata_path = '/kaggle/input/cifar10-python-in-csv/batches.meta' # change this path\nmetadata = unpickle(metadata_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:54:58.464674Z","iopub.execute_input":"2024-08-29T17:54:58.465300Z","iopub.status.idle":"2024-08-29T17:54:58.486121Z","shell.execute_reply.started":"2024-08-29T17:54:58.465266Z","shell.execute_reply":"2024-08-29T17:54:58.485271Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data_csv = pd.read_csv('/kaggle/input/cifar10-python-in-csv/train.csv')\ntest_data_csv = pd.read_csv('/kaggle/input/cifar10-python-in-csv/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:54:58.487303Z","iopub.execute_input":"2024-08-29T17:54:58.487599Z","iopub.status.idle":"2024-08-29T17:55:33.616714Z","shell.execute_reply.started":"2024-08-29T17:54:58.487569Z","shell.execute_reply":"2024-08-29T17:55:33.615666Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_data_csv.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-29T17:55:33.617947Z","iopub.execute_input":"2024-08-29T17:55:33.618279Z","iopub.status.idle":"2024-08-29T17:55:33.648314Z","shell.execute_reply.started":"2024-08-29T17:55:33.618245Z","shell.execute_reply":"2024-08-29T17:55:33.647346Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   pixel_0  pixel_1  pixel_2  pixel_3  pixel_4  pixel_5  pixel_6  pixel_7  \\\n0       59       43       50       68       98      119      139      145   \n1      154      126      105      102      125      155      172      180   \n2      255      253      253      253      253      253      253      253   \n3       28       37       38       42       44       40       40       24   \n4      170      168      177      183      181      177      181      184   \n\n   pixel_8  pixel_9  ...  pixel_3063  pixel_3064  pixel_3065  pixel_3066  \\\n0      149      149  ...          58          65          59          46   \n1      142      111  ...          42          67         101         122   \n2      253      253  ...          83          80          69          66   \n3       32       43  ...          39          59          42          44   \n4      189      189  ...          88          85          82          83   \n\n   pixel_3067  pixel_3068  pixel_3069  pixel_3070  pixel_3071  label  \n0          57         104         140          84          72      6  \n1         133         136         139         142         144      9  \n2          72          79          83          83          84      9  \n3          48          38          28          37          46      4  \n4          79          78          82          78          80      1  \n\n[5 rows x 3073 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel_0</th>\n      <th>pixel_1</th>\n      <th>pixel_2</th>\n      <th>pixel_3</th>\n      <th>pixel_4</th>\n      <th>pixel_5</th>\n      <th>pixel_6</th>\n      <th>pixel_7</th>\n      <th>pixel_8</th>\n      <th>pixel_9</th>\n      <th>...</th>\n      <th>pixel_3063</th>\n      <th>pixel_3064</th>\n      <th>pixel_3065</th>\n      <th>pixel_3066</th>\n      <th>pixel_3067</th>\n      <th>pixel_3068</th>\n      <th>pixel_3069</th>\n      <th>pixel_3070</th>\n      <th>pixel_3071</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59</td>\n      <td>43</td>\n      <td>50</td>\n      <td>68</td>\n      <td>98</td>\n      <td>119</td>\n      <td>139</td>\n      <td>145</td>\n      <td>149</td>\n      <td>149</td>\n      <td>...</td>\n      <td>58</td>\n      <td>65</td>\n      <td>59</td>\n      <td>46</td>\n      <td>57</td>\n      <td>104</td>\n      <td>140</td>\n      <td>84</td>\n      <td>72</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>154</td>\n      <td>126</td>\n      <td>105</td>\n      <td>102</td>\n      <td>125</td>\n      <td>155</td>\n      <td>172</td>\n      <td>180</td>\n      <td>142</td>\n      <td>111</td>\n      <td>...</td>\n      <td>42</td>\n      <td>67</td>\n      <td>101</td>\n      <td>122</td>\n      <td>133</td>\n      <td>136</td>\n      <td>139</td>\n      <td>142</td>\n      <td>144</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>255</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>253</td>\n      <td>...</td>\n      <td>83</td>\n      <td>80</td>\n      <td>69</td>\n      <td>66</td>\n      <td>72</td>\n      <td>79</td>\n      <td>83</td>\n      <td>83</td>\n      <td>84</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>28</td>\n      <td>37</td>\n      <td>38</td>\n      <td>42</td>\n      <td>44</td>\n      <td>40</td>\n      <td>40</td>\n      <td>24</td>\n      <td>32</td>\n      <td>43</td>\n      <td>...</td>\n      <td>39</td>\n      <td>59</td>\n      <td>42</td>\n      <td>44</td>\n      <td>48</td>\n      <td>38</td>\n      <td>28</td>\n      <td>37</td>\n      <td>46</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>170</td>\n      <td>168</td>\n      <td>177</td>\n      <td>183</td>\n      <td>181</td>\n      <td>177</td>\n      <td>181</td>\n      <td>184</td>\n      <td>189</td>\n      <td>189</td>\n      <td>...</td>\n      <td>88</td>\n      <td>85</td>\n      <td>82</td>\n      <td>83</td>\n      <td>79</td>\n      <td>78</td>\n      <td>82</td>\n      <td>78</td>\n      <td>80</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 3073 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data = []\ntest_data = []","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:17.923766Z","iopub.execute_input":"2024-08-29T18:06:17.924493Z","iopub.status.idle":"2024-08-29T18:06:17.948149Z","shell.execute_reply.started":"2024-08-29T18:06:17.924449Z","shell.execute_reply":"2024-08-29T18:06:17.947427Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"for i in tqdm(range(train_data_csv.shape[0])):\n    \n    row_data = np.array(train_data_csv.iloc[i], dtype = np.float64) \n    pixel_data = row_data[:3072]/255\n    label = int(row_data[-1])\n    image = torch.zeros(3, 32, 32)\n    image[0] = torch.tensor(pixel_data[:1024].reshape((32, 32)))\n    image[1] = torch.tensor(pixel_data[1024:2048].reshape((32, 32)))\n    image[2] = torch.tensor(pixel_data[2048:].reshape((32, 32))) \n    train_data.append((image, label))\n    \nfor i in tqdm(range(test_data_csv.shape[0])):\n    \n    row_data = np.array(train_data_csv.iloc[i], dtype = np.float64) \n    pixel_data = row_data[:3072]/255\n    label = int(row_data[-1])\n    image = torch.zeros(3, 32, 32)\n    image[0] = torch.tensor(pixel_data[:1024].reshape((32, 32)))\n    image[1] = torch.tensor(pixel_data[1024:2048].reshape((32, 32)))\n    image[2] = torch.tensor(pixel_data[2048:].reshape((32, 32))) \n    test_data.append((image, label))","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:18.090172Z","iopub.execute_input":"2024-08-29T18:06:18.090911Z","iopub.status.idle":"2024-08-29T18:06:32.850767Z","shell.execute_reply.started":"2024-08-29T18:06:18.090872Z","shell.execute_reply":"2024-08-29T18:06:32.849737Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:12<00:00, 4050.51it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:02<00:00, 4168.01it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def imshow(img):\n    \"\"\" Function to show an image \"\"\"\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.detach().cpu().numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')  # Hide the axes\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:34.869923Z","iopub.execute_input":"2024-08-29T18:06:34.870353Z","iopub.status.idle":"2024-08-29T18:06:34.875976Z","shell.execute_reply.started":"2024-08-29T18:06:34.870295Z","shell.execute_reply":"2024-08-29T18:06:34.875048Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder\nclass Encoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 16 * 16, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.mu = nn.Linear(1000, 128)\n        self.sigma = nn.Linear(1000, 128)\n\n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 16 * 16)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        mu = self.mu(x)\n        log_var = self.sigma(x)\n        return mu, log_var\n\nclass Decoder(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        self.layer3 = nn.Linear(128, 1000)\n        self.layer4 = nn.Linear(1000, 256 * 16 * 16)\n        self.deconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(128)\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(64)\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=3, padding=1)\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n\n    def forward(self, z):\n\n        z = F.relu(self.layer3(z))\n        z = F.relu(self.layer4(z))\n        z = z.view(-1, 256, 16, 16)  # Reshape to match the deconv input size\n        z = F.relu(self.bn5(self.upsample1(self.deconv1(z))))\n        z = F.relu(self.bn6(self.deconv2(z)))\n        z = torch.sigmoid((self.deconv3(z)))  # Added another deconv layer and upsampling\n        return z\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 16 * 16, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        \n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 16 * 16)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        x = F.relu(self.bn5(self.pen_final(x)))\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features\n\n# VAE_GAN class modification to include discriminator features in the loss function\nclass VAE_GAN(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real = self.discriminator(x)\n        disc_recon, features_recon = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior,_= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior","metadata":{"id":"iizGZeY9IQd6","execution":{"iopub.status.busy":"2024-08-29T18:06:35.055021Z","iopub.execute_input":"2024-08-29T18:06:35.055695Z","iopub.status.idle":"2024-08-29T18:06:35.086473Z","shell.execute_reply.started":"2024-08-29T18:06:35.055655Z","shell.execute_reply":"2024-08-29T18:06:35.085462Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"class CustomCNN(nn.Module):\n    \n    def __init__(self):\n        \n        super(CustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm1 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm2 = nn.BatchNorm2d(128)\n\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n\n        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm4 = nn.BatchNorm2d(512)\n\n        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm5 = nn.BatchNorm2d(512)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(512, 4096)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(4096, 10)\n        self.fc4 = nn.Linear(4096, 1)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.conv1(x))\n        x = F.leaky_relu(self.conv2(x))\n        x = self.pool1(x)\n        x = self.batch_norm1(x)\n        \n        x = F.leaky_relu(self.conv3(x))\n        x = F.leaky_relu(self.conv4(x))\n        x = self.pool2(x)\n        x = self.batch_norm2(x)\n\n        x = F.leaky_relu(self.conv5(x))\n        x = F.leaky_relu(self.conv6(x))\n        x = F.leaky_relu(self.conv7(x))\n        x = self.pool3(x)\n        x = self.batch_norm3(x)\n\n        x = F.leaky_relu(self.conv8(x))\n        x = F.leaky_relu(self.conv9(x))\n        x = F.leaky_relu(self.conv10(x))\n        x = self.pool4(x)\n        x = self.batch_norm4(x)\n\n        x = F.leaky_relu(self.conv11(x))\n        x = F.leaky_relu(self.conv12(x))\n        x = F.leaky_relu(self.conv13(x))\n        x = self.pool5(x)\n        x = self.batch_norm5(x)\n        \n        x = self.flatten(x)\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.dropout2(x)\n        features = torch.clone(x.cpu().detach()).to(device)\n        x = self.fc3(x)\n        return F.sigmoid(self.fc4(features)), features, x","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:35.324415Z","iopub.execute_input":"2024-08-29T18:06:35.325092Z","iopub.status.idle":"2024-08-29T18:06:35.347830Z","shell.execute_reply.started":"2024-08-29T18:06:35.325048Z","shell.execute_reply":"2024-08-29T18:06:35.346757Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"class Discriminator_multi(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n        \n        self.linear_layer = nn.Linear(256 * 16 * 16, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.disc_pen = nn.Linear(1000, 10)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n        self.bn6 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        self.final_dis = nn.Linear(10, 10)\n        \n    def forward(self, x):\n\n        x = F.leaky_relu(self.bn1(self.conv1(x)))\n        x = F.leaky_relu(self.bn2(self.conv2(x)))\n        x = F.leaky_relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 16 * 16)\n        x = F.leaky_relu(self.bn4(self.linear_layer(x)))\n        y = F.leaky_relu(self.bn6(self.disc_pen(x)))\n        x = F.leaky_relu(self.bn5(self.pen_final(x)))\n        y = F.softmax(self.final_dis(y), dim = 1)\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features, y\n    \n#     def forward(self, x):\n\n#         x = F.relu((self.conv1(x)))\n#         x = F.relu((self.conv2(x)))\n#         x = F.relu((self.conv3(x)))\n#         x = self.maxpool3(x)\n#         x = x.view(-1, 256 * 14 * 14)\n#         x = F.relu((self.linear_layer(x)))\n#         y = F.relu((self.disc_pen(x)))\n#         x = F.relu((self.pen_final(x)))\n#         y = F.softmax(self.final_dis(y), dim = 1)\n#         features = torch.clone(x.detach())\n#         return F.sigmoid(self.final_gen(x)), features, y","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:35.561066Z","iopub.execute_input":"2024-08-29T18:06:35.561500Z","iopub.status.idle":"2024-08-29T18:06:35.575579Z","shell.execute_reply.started":"2024-08-29T18:06:35.561460Z","shell.execute_reply":"2024-08-29T18:06:35.574541Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer_dis, optimizer_enc, optimizer_dec):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_dis': optimizer_dis.state_dict(),\n        'optimizer_enc': optimizer_enc.state_dict(),\n        'optimizer_dec': optimizer_dec.state_dict()\n    }\n    \n    torch.save(checkpoint, \"epoch{} checkpoint\".format(epoch))\n\n# Define a function to load a checkpoint\ndef load_checkpoint(model, optimizer_dis, optimizer_enc, optimizer_dec, filename):\n    \n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer_dis.load_state_dict(checkpoint['optimizer_dis'])\n    optimizer_enc.load_state_dict(checkpoint['optimizer_enc'])\n    optimizer_dec.load_state_dict(checkpoint['optimizer_dec'])\n    epoch = checkpoint['epoch']\n    return epoch","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:35.725684Z","iopub.execute_input":"2024-08-29T18:06:35.726352Z","iopub.status.idle":"2024-08-29T18:06:35.733705Z","shell.execute_reply.started":"2024-08-29T18:06:35.726311Z","shell.execute_reply":"2024-08-29T18:06:35.732687Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"def train(model, data_loader, optimizer_enc, optimizer_dec, optimizer_dis, epochs=5):\n    \n    try:\n        device = next(model.parameters()).device  # Get the device from the model's parameters\n        torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n        for p in model.parameters():\n            p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n        for epoch in range(epochs):\n            for batch_idx, (imgs, _) in enumerate(data_loader):\n                imgs = imgs.to(device)\n\n                # Zero the parameter gradients\n#                 optimizer_enc.zero_grad()\n#                 optimizer_dec.zero_grad()\n                optimizer_dis.zero_grad()\n\n                # Forward pass through VAE/GAN\n    #             print(model(imgs))\n                features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior = model(imgs)\n\n#                 mse_loss = nn.MSELoss(reduction = 'sum')\n                # Calculate losses\n                # Reconstruction loss and KL divergence for VAE\n                l_dis_like = mse_loss(features_real, features_recon)\n                l_prior = -0.1 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    #             print(disc_real)\n                l_gan = torch.sum(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1 - disc_prior))\n                \n    #             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n    #             enc_loss = l_dis_like + l_prior \n    #             dec_loss = l_dis_like - l_gan\n                disc_loss = -l_gan\n\n    #             print(l_gan.item())\n                disc_loss.backward()\n    #             enc_loss.backward(retain_graph = True)\n    #             dec_loss.backward()\n\n    #             optimizer_dis.step()\n    #             optimizer_enc.step()\n\n\n    #             print(max_norm)\n    # Calculate the total norm of all gradients\n#                 total_norm = 0\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         total_norm += torch.norm(param.grad).item() ** 2\n#                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n#                 # Clip gradients if the total norm exceeds a threshold\n#                 max_norm = 100  # Adjust as needed\n#                 if total_norm > max_norm:\n#                     for name, param in model.discriminator.named_parameters():\n#                         if param.grad is not None:\n#                             param.grad *= max_norm / total_norm\n\n                optimizer_dis.step()\n\n                if(batch_idx % 50) == 0:             \n                    total_norm = 0\n                    for name, param in model.discriminator.named_parameters():\n                        if param.grad is not None:\n                            total_norm += torch.norm(param.grad).item() ** 2\n                    total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n                    print(total_norm)\n\n                if batch_idx % 1000 == 0:\n\n                    print(\"epoch:{}\".format(epoch))\n                    print(\"disc_loss:\", disc_loss.item())\n                    print('Original Images')\n                    imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                    # Pass images through the model to get the reconstructions\n                    with torch.no_grad():\n\n                        imgs = imgs.to(device)\n                        mu, log_var = model.encoder(imgs)\n                        noise = torch.rand_like(log_var)\n                        z = mu + noise * torch.exp(0.5 * log_var)\n                        recon_images = model.decoder(z)\n                        recon_images = recon_images.cpu()\n\n                    print('Reconstructed Images')\n                    imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n            if epoch%10 == 0:\n                torch.save(model.discriminator.state_dict(), 'disc.pt')\n    \n    except:\n        return","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:35.882131Z","iopub.execute_input":"2024-08-29T18:06:35.882558Z","iopub.status.idle":"2024-08-29T18:06:35.906171Z","shell.execute_reply.started":"2024-08-29T18:06:35.882519Z","shell.execute_reply":"2024-08-29T18:06:35.904112Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"class VAE_GAN_multi(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN_multi, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real, class_real = self.discriminator(x)\n        disc_recon, features_recon, _ = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior, _, _= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:36.199133Z","iopub.execute_input":"2024-08-29T18:06:36.199732Z","iopub.status.idle":"2024-08-29T18:06:36.208983Z","shell.execute_reply.started":"2024-08-29T18:06:36.199687Z","shell.execute_reply":"2024-08-29T18:06:36.207902Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"def train_multi(model, data_loader, optimizer_dis, optimizer_enc, optimizer_dec,  epochs=5):\n    \n    device = next(model.parameters()).device  # Get the device from the model's parameters\n    torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n#         for p in model.parameters():\n#             p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n    for epoch in range(epochs):\n        for batch_idx, (imgs, labels) in enumerate(data_loader):\n            \n            imgs = imgs.to(device)\n            labels_one_hot = F.one_hot(labels, num_classes=10).float().to(device)\n\n            # Zero the parameter gradients\n            optimizer_dis.zero_grad()\n            optimizer_enc.zero_grad()\n            optimizer_dec.zero_grad()\n\n\n            # Forward pass through VAE/GAN\n#             print(model(imgs))\n            features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = model(imgs)\n\n            cross_entropy_loss = nn.CrossEntropyLoss()\n            # Calculate losses\n            # Reconstruction loss and KL divergence for VAE\n# #             l_dis_like = torch.mean(torch.sum((features_real - features_recon)**2, dim = 1))\n# #             l_prior = -0.01 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n#             print(disc_real)\n            l_gan = torch.mean(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1-disc_prior))\n            l_class = cross_entropy_loss(class_real, labels_one_hot) \n\n#             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n#             enc_loss = l_dis_like + l_prior \n#             dec_loss = l_dis_like + l_gan\n            disc_loss = 0.7 * -l_gan + 0.3 * l_class\n            \n            try:\n#                 enc_loss.backward(retain_graph = True)\n#                 dec_loss.backward(retain_graph = True)\n                disc_loss.backward()\n            \n    \n#                 optimizer_enc.step()\n#                 optimizer_dec.step()\n                optimizer_dis.step()\n                \n            except: \n                \n                print('disc_real:', torch.mean(disc_real))\n                print('disc_recon:', torch.mean(disc_recon))\n                print('disc_prior:', torch.mean(disc_prior))\n                print('true_class_prob:', torch.exp(-l_class))\n                print('\\n')\n                \n                epsilon = 1e-4\n                for param in model.discriminator.parameters():\n                    if param.requires_grad:\n                        noise = torch.randn_like(param) * epsilon\n                        param.data += noise\n\n# Calculate the total norm of all gradients\n\n            if batch_idx % 500 == 0: \n#                 print(l_gan.item())\n                print('disc_real:', torch.mean(disc_real))\n                print('disc_recon:', torch.mean( disc_recon))\n                print('disc_prior:',torch.mean( disc_prior))\n                print('true_class_prob:', torch.exp(-l_class))\n                print('\\n')\n                \n#             try:    \n#                 disc_loss.backward()\n                \n#             except: \n                \n#                 print('disc_real:', torch.mean(disc_real))\n#                 print('disc_recon:', torch.mean(disc_recon))\n#                 print('disc_prior:',torch.mean(disc_prior))\n#                 print('true_class_prob:', torch.exp(-l_class))\n#                 print(\"-l_gan, l_class:\", -l_gan.item(), \" \", l_class.item())\n#                 print('\\n')\n            \n#             total_norm = 0\n#             for name, param in model.discriminator.named_parameters():\n#                 if param.grad is not None:\n#                     total_norm += torch.norm(param.grad).item() ** 2\n#             total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n#             # Clip gradients if the total norm exceeds a threshold\n#             max_norm = 10 * (0.99)**epoch # Adjust as needed\n#             if total_norm > max_norm:\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         param.grad *= max_norm / total_norm\n                        \n#             if batch_idx % 50 == 0: \n#                 total_norm = 0\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         total_norm += torch.norm(param.grad).item() ** 2\n                        \n#                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n#                 print('grad_norm:', total_norm)\n                \n            \n            imgs = imgs.to('cpu')\n        \n            \n            if batch_idx % 1000 == 0:\n\n                print(\"epoch:{}\".format(epoch))\n                print(\"-l_gan, l_class:\", -l_gan.item(), \" \", l_class.item())\n                print('Original Images')\n                imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                # Pass images through the model to get the reconstructions\n                with torch.no_grad():\n\n                    imgs = imgs.to(device)\n                    mu, log_var = model.encoder(imgs)\n                    noise = torch.rand_like(log_var)\n                    z = mu + noise * torch.exp(0.5 * log_var)\n                    recon_images = model.decoder(z)\n                    recon_images = recon_images.cpu()\n                    imgs = imgs.to('cpu')\n\n                print('Reconstructed Images')\n                imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n        if epoch%10 == 0:\n            torch.save(model.discriminator.state_dict(), 'disc_multi.pt')","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:36.505884Z","iopub.execute_input":"2024-08-29T18:06:36.506269Z","iopub.status.idle":"2024-08-29T18:06:36.525442Z","shell.execute_reply.started":"2024-08-29T18:06:36.506235Z","shell.execute_reply":"2024-08-29T18:06:36.524480Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# Initialize the models and optimizers\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = Encoder().to(device)\ndecoder = Decoder().to(device)\n# discriminator = Discriminator().to(device)","metadata":{"id":"j9P7UnA7z7l9","outputId":"42310e36-6714-4efe-8671-5a184f0a47d0","execution":{"iopub.status.busy":"2024-08-29T18:06:37.333774Z","iopub.execute_input":"2024-08-29T18:06:37.334548Z","iopub.status.idle":"2024-08-29T18:06:38.589257Z","shell.execute_reply.started":"2024-08-29T18:06:37.334509Z","shell.execute_reply":"2024-08-29T18:06:38.588396Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# vae_gan = VAE_GAN(encoder, decoder, discriminator).to(device)\noptimizer_enc = Adam(encoder.parameters(), lr = 3e-5)\noptimizer_dec = Adam(decoder.parameters(), lr = 3e-5)\n# optimizer_dis = Adam(discriminator.parameters(), lr=6e-5)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:06:38.591098Z","iopub.execute_input":"2024-08-29T18:06:38.591536Z","iopub.status.idle":"2024-08-29T18:06:38.597087Z","shell.execute_reply.started":"2024-08-29T18:06:38.591488Z","shell.execute_reply":"2024-08-29T18:06:38.596161Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"discriminator_multi = Discriminator_multi().to(device)\nvae_gan_multi = VAE_GAN_multi(encoder, decoder, discriminator_multi).to(device)\noptimizer_dis = Adam(discriminator_multi.parameters(), lr = 3e-5)\ndata_loader = DataLoader(train_data, batch_size = 4, shuffle = True)","metadata":{"id":"iHqyUNNhY_CX","execution":{"iopub.status.busy":"2024-08-29T18:06:38.598120Z","iopub.execute_input":"2024-08-29T18:06:38.598384Z","iopub.status.idle":"2024-08-29T18:06:39.226091Z","shell.execute_reply.started":"2024-08-29T18:06:38.598355Z","shell.execute_reply":"2024-08-29T18:06:39.225300Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.encoder.eval()\nvae_gan_multi.decoder.eval()\nvae_gan_multi.discriminator.train()\ntrain_multi(vae_gan_multi, data_loader, optimizer_dis, None, None,  epochs = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.eval()\n# vae_gan_multi.discriminator.train()\n# vae_gan_multi.to('cpu')\nfor imgs, labels in test_loader:\n    imgs = imgs.to(device)\n    features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = vae_gan_multi(imgs)\n    print(torch.mean(disc_real), torch.mean(disc_recon), torch.mean(disc_prior))\n    imgs = imgs.to('cpu')\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for imgs, _ in test_loader: \n#     with torch.no_grad():\n\n#         imgs = imgs.to(device)\n#         mu, log_var = encoder(imgs)\n#         noise = torch.rand_like(log_var)\n#         z = mu + noise * torch.exp(0.5 * log_var)\n#         recon_images = decoder(z)\n#         recon_images = recon_images.cpu()\n#         imgs = imgs.to('cpu')\n\n#     print('Reconstructed Images')\n#     imshow(torchvision.utils.make_grid(recon_images[:10]))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:31.647932Z","iopub.execute_input":"2024-06-17T17:08:31.648221Z","iopub.status.idle":"2024-06-17T17:08:40.542136Z","shell.execute_reply.started":"2024-06-17T17:08:31.648177Z","shell.execute_reply":"2024-06-17T17:08:40.540991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Assuming `train_dataset` is a PyTorch dataset where each item is a tuple (image, label)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Stack all images into a single tensor\nall_images = torch.stack([train_data[i][0].squeeze().flatten() for i in range(len(train_data))]).to(device)\nnum_items = all_images.size(0)\n\nclosest_indices = []\n\n# Compute pairwise distances using broadcasting\nwith torch.no_grad():\n    # Compute the L2 distance between each pair of \"\"images\n    distances = torch.cdist(all_images, all_images, p=2)  # Shape: (num_items, num_items)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:14:37.053048Z","iopub.execute_input":"2024-08-29T18:14:37.053890Z","iopub.status.idle":"2024-08-29T18:14:38.071059Z","shell.execute_reply.started":"2024-08-29T18:14:37.053849Z","shell.execute_reply":"2024-08-29T18:14:38.070195Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():   \n    for i in tqdm(range(num_items)):\n        # Exclude the distance to itself by setting it to a high value\n        distances[i, i] = float('inf')\n        # Get the indices of the 10 smallest distances\n        _, indices = torch.topk(distances[i], 10, largest=False)\n        indices = indices.to('cpu')\n        closest_indices.append(indices.cpu().tolist())\n\nall_images = all_images.to('cpu')  # Move the tensor back to CPU if needed\ndistances = distances.to('cpu') ","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:14:46.912186Z","iopub.execute_input":"2024-08-29T18:14:46.912552Z","iopub.status.idle":"2024-08-29T18:15:03.591153Z","shell.execute_reply.started":"2024-08-29T18:14:46.912518Z","shell.execute_reply":"2024-08-29T18:15:03.590332Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:07<00:00, 6372.24it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom scipy.linalg import lstsq\n\ndef approximate_target_matrix(target, matrices):\n    # Flatten the target matrix\n    target_flat = target.flatten()\n    \n    # Stack flattened matrices as columns\n    matrix_flat_stack = np.stack([matrix.flatten() for matrix in matrices], axis=1)\n    \n    # Solve the least squares problem\n    coefficients, _, _, _ = lstsq(matrix_flat_stack, target_flat)\n    \n    return coefficients\n\ncoefficients = []\nfor i in tqdm(range(num_items)):\n    \n    present = train_data[i][0].squeeze().numpy()\n    indices = closest_indices[i] \n    consts = []\n    \n    for index in indices:\n        consts.append(train_data[index][0].squeeze().numpy()) \n        \n    coeffs = approximate_target_matrix(present, consts)\n    coefficients.append(coeffs)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:15:29.853349Z","iopub.execute_input":"2024-08-29T18:15:29.853740Z","iopub.status.idle":"2024-08-29T18:16:15.902144Z","shell.execute_reply.started":"2024-08-29T18:15:29.853703Z","shell.execute_reply":"2024-08-29T18:16:15.901011Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:46<00:00, 1086.14it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass CustomImageDataset(Dataset):\n    \n    def __init__(self, train_dataset, closest_indices, coefficients):\n        \"\"\"\n        Args:\n            images (list of tensors): List of image tensors.\n            closest_indices (list of lists): List where each element is a list of indices of the closest images.\n            coefficients (list of lists): List where each element is a list of coefficients for the closest images.\n        \"\"\"\n        self.dataset = train_dataset\n        self.closest_indices = closest_indices\n        self.coefficients = coefficients\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        \n        item = self.dataset[idx]\n        closest_images = [self.dataset[i][0] for i in self.closest_indices[idx]]\n        coeffs = self.coefficients[idx]\n\n        closest_images_tensor = torch.stack(closest_images)\n        coeffs_tensor = torch.tensor(coeffs, dtype=torch.float)\n        \n        image = item[0]\n        label = item[1]\n        \n        return image, label, closest_images_tensor, coeffs_tensor","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:18:24.609438Z","iopub.execute_input":"2024-08-29T18:18:24.609830Z","iopub.status.idle":"2024-08-29T18:18:24.618581Z","shell.execute_reply.started":"2024-08-29T18:18:24.609792Z","shell.execute_reply":"2024-08-29T18:18:24.617542Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 24\nnew_dataset = CustomImageDataset(train_data, closest_indices, coefficients)\nnew_dataloader = DataLoader(new_dataset, shuffle = True, batch_size = BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:21:51.764599Z","iopub.execute_input":"2024-08-29T18:21:51.765576Z","iopub.status.idle":"2024-08-29T18:21:51.770529Z","shell.execute_reply.started":"2024-08-29T18:21:51.765530Z","shell.execute_reply":"2024-08-29T18:21:51.769532Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"# import gc\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.608849Z","iopub.execute_input":"2024-06-13T16:49:01.609113Z","iopub.status.idle":"2024-06-13T16:49:01.777623Z","shell.execute_reply.started":"2024-06-13T16:49:01.609064Z","shell.execute_reply":"2024-06-13T16:49:01.776696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import gc\n\n# def get_cuda_tensors():\n#     cuda_tensors = []\n#     for obj in gc.get_objects():\n#         try:\n#             if torch.is_tensor(obj) and obj.is_cuda:\n#                 cuda_tensors.append(obj)\n#         except:\n#             pass\n#     return cuda_tensors\n\n# # Usage example\n# cuda_tensors = get_cuda_tensors()\n# for tensor in cuda_tensors:\n#     print(tensor.size(), tensor.device)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.779165Z","iopub.execute_input":"2024-06-13T16:49:01.779606Z","iopub.status.idle":"2024-06-13T16:49:01.990351Z","shell.execute_reply.started":"2024-06-13T16:49:01.779571Z","shell.execute_reply":"2024-06-13T16:49:01.989449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\ncriterion = nn.MSELoss(reduction = 'sum')\nfor epoch in range(epochs):\n    \n    total_loss = 0\n    count = 0\n    for image, _ , closest_images_tensor, coeffs_tensor in new_dataloader:\n        \n        optimizer_enc.zero_grad()\n        optimizer_dec.zero_grad()\n        \n        closest_images_tensor = closest_images_tensor.to(device)\n        coeffs_tensor = coeffs_tensor.to(device)\n        image = image.to(device)\n        \n        mu, log_var = encoder(image)\n        noise = torch.rand_like(log_var)\n        my_z = mu + noise * torch.exp(0.5 * log_var) #[24, 10]\n        recons = decoder(my_z)\n        \n        # closest_images_tensor of shape [24, 10, 3, 32, 32]\n        batch_size = closest_images_tensor.shape[0]\n        dost_sums = []\n        \n        for i in range(batch_size):\n            sub_batch = closest_images_tensor[i, :, : , : , :] # [1, 10, 3, 32, 32]\n            sub_batch = sub_batch.squeeze(0) # [10, 32, 32, 32] \n            mu, log_var = encoder(sub_batch) \n            noise = torch.rand_like(log_var)\n            z = mu + noise * torch.exp(0.5 * log_var) # [128, 10]\n            dost_recons_sum = z.T @ coeffs_tensor[i] # [128, 1]\n            dost_sums.append(dost_recons_sum.unsqueeze(1).T) \n        \n        dost_recons_sum = torch.stack(dost_sums).squeeze() #[batch_size, 128]\n        \n        loss1 = criterion(image, recons)/BATCH_SIZE\n        loss2 = torch.mean(torch.sum((my_z - dost_recons_sum)**2, axis = 1))\n        loss = loss1 + 0.1*loss2 - 5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        \n        loss.backward()\n        optimizer_enc.step()\n        optimizer_dec.step()\n        total_loss += loss\n        count +=1\n        \n        closest_images_tensor = closest_images_tensor.to('cpu')\n        coeffs_tensor = coeffs_tensor.to('cpu')\n        image = image.to('cpu')\n        \n    print('Original Images')\n    imshow(torchvision.utils.make_grid(image[:10]))\n\n    # Pass images through the model to get the reconstructions\n    with torch.no_grad():\n\n        image = image.to(device)\n        mu, log_var = encoder(image)\n        noise = torch.rand_like(log_var)\n        z = mu + noise * torch.exp(0.5 * log_var)\n        recon_images = decoder(z)\n        recon_images = recon_images.cpu()\n        image = image.to(device)\n\n    print('Reconstructed Images')\n    imshow(torchvision.utils.make_grid(recon_images[:10]))\n    print(\"The epoch is {} and the loss is {}\".format(epoch, total_loss/count))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(encoder.state_dict(), 'encoder.pt')\n# torch.save(decoder.state_dict(), 'decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:48:44.564411Z","iopub.execute_input":"2024-06-09T10:48:44.564671Z","iopub.status.idle":"2024-06-09T10:48:44.577123Z","shell.execute_reply.started":"2024-06-09T10:48:44.564643Z","shell.execute_reply":"2024-06-09T10:48:44.576397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}