{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65638,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":54759,"modelId":60106},{"sourceId":65964,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55008,"modelId":59792},{"sourceId":65966,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":55010,"modelId":59803}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision","metadata":{"id":"XqUJotsbIIoy","outputId":"ff786056-fcb5-408d-b6c3-802a58549b89","execution":{"iopub.status.busy":"2024-06-17T17:08:00.817282Z","iopub.execute_input":"2024-06-17T17:08:00.817716Z","iopub.status.idle":"2024-06-17T17:08:14.064263Z","shell.execute_reply.started":"2024-06-17T17:08:00.817683Z","shell.execute_reply":"2024-06-17T17:08:14.063063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:14.065817Z","iopub.execute_input":"2024-06-17T17:08:14.066176Z","iopub.status.idle":"2024-06-17T17:08:14.071962Z","shell.execute_reply.started":"2024-06-17T17:08:14.066145Z","shell.execute_reply":"2024-06-17T17:08:14.071064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define the transformation to include normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.130,), ( 0.308,))  # Normalize with mean=0.5 and std=0.5\n])\n\n# Data loading\ntrain_dataset = datasets.FashionMNIST(root = './data', train=True, download=True, transform=transform)\ntest_dataset = datasets.FashionMNIST(root = './data', train=False, download=True, transform=transform)\ndata_loader = DataLoader(train_dataset, batch_size = 256, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size = 256, shuffle = True)","metadata":{"id":"GeCHvUJIfujm","execution":{"iopub.status.busy":"2024-06-17T17:08:17.087623Z","iopub.execute_input":"2024-06-17T17:08:17.088237Z","iopub.status.idle":"2024-06-17T17:08:17.172569Z","shell.execute_reply.started":"2024-06-17T17:08:17.088182Z","shell.execute_reply":"2024-06-17T17:08:17.171737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(img):\n    \"\"\" Function to show an image \"\"\"\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.detach().cpu().numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.axis('off')  # Hide the axes\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:18.079182Z","iopub.execute_input":"2024-06-17T17:08:18.080035Z","iopub.status.idle":"2024-06-17T17:08:18.085600Z","shell.execute_reply.started":"2024-06-17T17:08:18.080002Z","shell.execute_reply":"2024-06-17T17:08:18.084617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the Encoder\nclass Encoder(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.mu = nn.Linear(1000, 128)\n        self.sigma = nn.Linear(1000, 128)\n\n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        mu = self.mu(x)\n        log_var = self.sigma(x)\n        return mu, log_var\n\nclass Decoder(nn.Module):\n    \n    def __init__(self):\n        \n        super().__init__()\n        self.layer3 = nn.Linear(128, 1000)\n        self.layer4 = nn.Linear(1000, 256 * 14 * 14)\n        self.deconv1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, padding=1)\n        self.bn5 = nn.BatchNorm2d(128)\n        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n        self.bn6 = nn.BatchNorm2d(64)\n        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.deconv3 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n\n    def forward(self, z):\n\n        z = F.relu(self.layer3(z))\n        z = F.relu(self.layer4(z))\n        z = z.view(-1, 256, 14, 14)  # Reshape to match the deconv input size\n        z = F.relu(self.bn5(self.upsample1(self.deconv1(z))))\n        z = F.relu(self.bn6(self.deconv2(z)))\n        z = torch.sigmoid((self.deconv3(z)))  # Added another deconv layer and upsampling\n        return z\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        \n    def forward(self, x):\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.relu(self.bn4(self.linear_layer(x)))\n        x = F.relu(self.bn5(self.pen_final(x)))\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features\n\n# VAE_GAN class modification to include discriminator features in the loss function\nclass VAE_GAN(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real = self.discriminator(x)\n        disc_recon, features_recon = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior,_= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior","metadata":{"id":"iizGZeY9IQd6","execution":{"iopub.status.busy":"2024-06-17T17:08:19.821720Z","iopub.execute_input":"2024-06-17T17:08:19.822077Z","iopub.status.idle":"2024-06-17T17:08:19.852399Z","shell.execute_reply.started":"2024-06-17T17:08:19.822048Z","shell.execute_reply":"2024-06-17T17:08:19.851378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomCNN(nn.Module):\n    \n    def __init__(self):\n        \n        super(CustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm1 = nn.BatchNorm2d(64)\n\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm2 = nn.BatchNorm2d(128)\n\n        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n\n        self.conv8 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv9 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm4 = nn.BatchNorm2d(512)\n\n        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=1)  # default stride is 2\n        self.batch_norm5 = nn.BatchNorm2d(512)\n\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(512, 4096)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(4096, 4096)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(4096, 10)\n        self.fc4 = nn.Linear(4096, 1)\n\n    def forward(self, x):\n        x = F.leaky_relu(self.conv1(x))\n        x = F.leaky_relu(self.conv2(x))\n        x = self.pool1(x)\n        x = self.batch_norm1(x)\n        \n        x = F.leaky_relu(self.conv3(x))\n        x = F.leaky_relu(self.conv4(x))\n        x = self.pool2(x)\n        x = self.batch_norm2(x)\n\n        x = F.leaky_relu(self.conv5(x))\n        x = F.leaky_relu(self.conv6(x))\n        x = F.leaky_relu(self.conv7(x))\n        x = self.pool3(x)\n        x = self.batch_norm3(x)\n\n        x = F.leaky_relu(self.conv8(x))\n        x = F.leaky_relu(self.conv9(x))\n        x = F.leaky_relu(self.conv10(x))\n        x = self.pool4(x)\n        x = self.batch_norm4(x)\n\n        x = F.leaky_relu(self.conv11(x))\n        x = F.leaky_relu(self.conv12(x))\n        x = F.leaky_relu(self.conv13(x))\n        x = self.pool5(x)\n        x = self.batch_norm5(x)\n        \n        x = self.flatten(x)\n        x = F.leaky_relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.fc2(x))\n        x = self.dropout2(x)\n        features = torch.clone(x.cpu().detach()).to(device)\n        x = self.fc3(x)\n        return F.sigmoid(self.fc4(features)), features, x","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:20.483522Z","iopub.execute_input":"2024-06-17T17:08:20.483946Z","iopub.status.idle":"2024-06-17T17:08:20.504207Z","shell.execute_reply.started":"2024-06-17T17:08:20.483910Z","shell.execute_reply":"2024-06-17T17:08:20.503146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator_multi(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n\n        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2, padding=0, stride=2)\n        \n        self.linear_layer = nn.Linear(256 * 14 * 14, 1000)\n        self.bn4 = nn.BatchNorm1d(1000)\n        self.disc_pen = nn.Linear(1000, 10)\n        self.pen_final = nn.Linear(1000, 10)\n        self.bn5 = nn.BatchNorm1d(10)\n        self.bn6 = nn.BatchNorm1d(10)\n#         self.final_dis = nn.Linear(10, 10)\n        self.final_gen = nn.Linear(10, 1)\n        self.final_dis = nn.Linear(10, 10)\n        \n    def forward(self, x):\n\n        x = F.leaky_relu(self.bn1(self.conv1(x)))\n        x = F.leaky_relu(self.bn2(self.conv2(x)))\n        x = F.leaky_relu(self.bn3(self.conv3(x)))\n        x = self.maxpool3(x)\n        x = x.view(-1, 256 * 14 * 14)\n        x = F.leaky_relu(self.bn4(self.linear_layer(x)))\n        y = F.leaky_relu(self.bn6(self.disc_pen(x)))\n        x = F.leaky_relu(self.bn5(self.pen_final(x)))\n        y = F.softmax(self.final_dis(y), dim = 1)\n        features = torch.clone(x.detach())\n        return F.sigmoid(self.final_gen(x)), features, y\n    \n#     def forward(self, x):\n\n#         x = F.relu((self.conv1(x)))\n#         x = F.relu((self.conv2(x)))\n#         x = F.relu((self.conv3(x)))\n#         x = self.maxpool3(x)\n#         x = x.view(-1, 256 * 14 * 14)\n#         x = F.relu((self.linear_layer(x)))\n#         y = F.relu((self.disc_pen(x)))\n#         x = F.relu((self.pen_final(x)))\n#         y = F.softmax(self.final_dis(y), dim = 1)\n#         features = torch.clone(x.detach())\n#         return F.sigmoid(self.final_gen(x)), features, y","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:20.897225Z","iopub.execute_input":"2024-06-17T17:08:20.898013Z","iopub.status.idle":"2024-06-17T17:08:20.917648Z","shell.execute_reply.started":"2024-06-17T17:08:20.897975Z","shell.execute_reply":"2024-06-17T17:08:20.916711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(epoch, model, optimizer_dis, optimizer_enc, optimizer_dec):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_dis': optimizer_dis.state_dict(),\n        'optimizer_enc': optimizer_enc.state_dict(),\n        'optimizer_dec': optimizer_dec.state_dict()\n    }\n    \n    torch.save(checkpoint, \"epoch{} checkpoint\".format(epoch))\n\n# Define a function to load a checkpoint\ndef load_checkpoint(model, optimizer_dis, optimizer_enc, optimizer_dec, filename):\n    \n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer_dis.load_state_dict(checkpoint['optimizer_dis'])\n    optimizer_enc.load_state_dict(checkpoint['optimizer_enc'])\n    optimizer_dec.load_state_dict(checkpoint['optimizer_dec'])\n    epoch = checkpoint['epoch']\n    return epoch","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:21.519556Z","iopub.execute_input":"2024-06-17T17:08:21.520238Z","iopub.status.idle":"2024-06-17T17:08:21.526996Z","shell.execute_reply.started":"2024-06-17T17:08:21.520183Z","shell.execute_reply":"2024-06-17T17:08:21.525996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, data_loader, optimizer_enc, optimizer_dec, optimizer_dis, epochs=5):\n    \n    try:\n        device = next(model.parameters()).device  # Get the device from the model's parameters\n        torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n        for p in model.parameters():\n            p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n        for epoch in range(epochs):\n            for batch_idx, (imgs, _) in enumerate(data_loader):\n                imgs = imgs.to(device)\n\n                # Zero the parameter gradients\n#                 optimizer_enc.zero_grad()\n#                 optimizer_dec.zero_grad()\n                optimizer_dis.zero_grad()\n\n                # Forward pass through VAE/GAN\n    #             print(model(imgs))\n                features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior = model(imgs)\n\n#                 mse_loss = nn.MSELoss(reduction = 'sum')\n                # Calculate losses\n                # Reconstruction loss and KL divergence for VAE\n                l_dis_like = mse_loss(features_real, features_recon)\n                l_prior = -0.1 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n    #             print(disc_real)\n                l_gan = torch.sum(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1 - disc_prior))\n                \n    #             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n    #             enc_loss = l_dis_like + l_prior \n    #             dec_loss = l_dis_like - l_gan\n                disc_loss = -l_gan\n\n    #             print(l_gan.item())\n                disc_loss.backward()\n    #             enc_loss.backward(retain_graph = True)\n    #             dec_loss.backward()\n\n    #             optimizer_dis.step()\n    #             optimizer_enc.step()\n\n\n    #             print(max_norm)\n    # Calculate the total norm of all gradients\n#                 total_norm = 0\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         total_norm += torch.norm(param.grad).item() ** 2\n#                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n#                 # Clip gradients if the total norm exceeds a threshold\n#                 max_norm = 100  # Adjust as needed\n#                 if total_norm > max_norm:\n#                     for name, param in model.discriminator.named_parameters():\n#                         if param.grad is not None:\n#                             param.grad *= max_norm / total_norm\n\n                optimizer_dis.step()\n\n                if(batch_idx % 50) == 0:             \n                    total_norm = 0\n                    for name, param in model.discriminator.named_parameters():\n                        if param.grad is not None:\n                            total_norm += torch.norm(param.grad).item() ** 2\n                    total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n                    print(total_norm)\n\n                if batch_idx % 1000 == 0:\n\n                    print(\"epoch:{}\".format(epoch))\n                    print(\"disc_loss:\", disc_loss.item())\n                    print('Original Images')\n                    imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                    # Pass images through the model to get the reconstructions\n                    with torch.no_grad():\n\n                        imgs = imgs.to(device)\n                        mu, log_var = model.encoder(imgs)\n                        noise = torch.rand_like(log_var)\n                        z = mu + noise * torch.exp(0.5 * log_var)\n                        recon_images = model.decoder(z)\n                        recon_images = recon_images.cpu()\n\n                    print('Reconstructed Images')\n                    imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n            if epoch%10 == 0:\n                torch.save(model.discriminator.state_dict(), 'disc.pt')\n    \n    except:\n        return","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:22.097365Z","iopub.execute_input":"2024-06-17T17:08:22.097701Z","iopub.status.idle":"2024-06-17T17:08:22.113177Z","shell.execute_reply.started":"2024-06-17T17:08:22.097675Z","shell.execute_reply":"2024-06-17T17:08:22.112109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VAE_GAN_multi(nn.Module):\n    \n    def __init__(self, encoder, decoder, discriminator):\n        super(VAE_GAN_multi, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.discriminator = discriminator\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        \n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        recon_x = self.decoder(z)\n        disc_real, features_real, class_real = self.discriminator(x)\n        disc_recon, features_recon, _ = self.discriminator(recon_x)\n        # Sample new data from the prior for discriminator\n\n        z_prior = torch.randn_like(z)\n        prior_x = self.decoder(z_prior)\n        disc_prior, _, _= self.discriminator(prior_x.detach())  # Detach to prevent gradients to decoder\n        return features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:22.759606Z","iopub.execute_input":"2024-06-17T17:08:22.759964Z","iopub.status.idle":"2024-06-17T17:08:22.768541Z","shell.execute_reply.started":"2024-06-17T17:08:22.759934Z","shell.execute_reply":"2024-06-17T17:08:22.767538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_multi(model, data_loader, optimizer_dis, optimizer_enc, optimizer_dec,  epochs=5):\n    \n    device = next(model.parameters()).device  # Get the device from the model's parameters\n    torch.autograd.set_detect_anomaly(True)  # Enable anomaly detection\n#         for p in model.parameters():\n#             p.register_hook(lambda grad: torch.clamp(grad, -300, 300))\n\n    for epoch in range(epochs):\n        for batch_idx, (imgs, labels) in enumerate(data_loader):\n            imgs = imgs.to(device)\n            labels_one_hot = F.one_hot(labels, num_classes=10).float().to(device)\n\n            # Zero the parameter gradients\n            optimizer_dis.zero_grad()\n# # #             optimizer_enc.zero_grad()\n# # #             optimizer_dec.zero_grad()\n\n\n            # Forward pass through VAE/GAN\n#             print(model(imgs))\n            features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = model(imgs)\n\n            cross_entropy_loss = nn.CrossEntropyLoss()\n            # Calculate losses\n            # Reconstruction loss and KL divergence for VAE\n# #             l_dis_like = torch.mean(torch.sum((features_real - features_recon)**2, dim = 1))\n# #             l_prior = -0.01 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n\n#             print(disc_real)\n            l_gan = torch.mean(torch.log(disc_real) + torch.log(1 - disc_recon) + torch.log(1-disc_prior))\n            l_class = cross_entropy_loss(class_real, labels_one_hot) \n\n#             l_gan = torch.sum(-torch.log(disc_real) + torch.log(disc_prior))\n#             enc_loss = l_dis_like + l_prior \n#             dec_loss = l_dis_like + l_gan\n            disc_loss = 0.7 * -l_gan + 0.3 * l_class\n            \n            try:\n#                 enc_loss.backward(retain_graph = True)\n#                 dec_loss.backward(retain_graph = True)\n                disc_loss.backward()\n            \n    \n#                 optimizer_enc.step()\n#                 optimizer_dec.step()\n                optimizer_dis.step()\n                \n            except: \n                \n                print('disc_real:', torch.mean(disc_real))\n                print('disc_recon:', torch.mean(disc_recon))\n                print('disc_prior:', torch.mean(disc_prior))\n                print('true_class_prob:', torch.exp(-l_class))\n                print('\\n')\n                \n                epsilon = 1e-4\n                for param in model.discriminator.parameters():\n                    if param.requires_grad:\n                        noise = torch.randn_like(param) * epsilon\n                        param.data += noise\n\n# Calculate the total norm of all gradients\n\n            if batch_idx % 500 == 0: \n#                 print(l_gan.item())\n                print('disc_real:', torch.mean(disc_real))\n                print('disc_recon:', torch.mean( disc_recon))\n                print('disc_prior:',torch.mean( disc_prior))\n                print('true_class_prob:', torch.exp(-l_class))\n                print('\\n')\n                \n#             try:    \n#                 disc_loss.backward()\n                \n#             except: \n                \n#                 print('disc_real:', torch.mean(disc_real))\n#                 print('disc_recon:', torch.mean(disc_recon))\n#                 print('disc_prior:',torch.mean(disc_prior))\n#                 print('true_class_prob:', torch.exp(-l_class))\n#                 print(\"-l_gan, l_class:\", -l_gan.item(), \" \", l_class.item())\n#                 print('\\n')\n            \n#             total_norm = 0\n#             for name, param in model.discriminator.named_parameters():\n#                 if param.grad is not None:\n#                     total_norm += torch.norm(param.grad).item() ** 2\n#             total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n\n#             # Clip gradients if the total norm exceeds a threshold\n#             max_norm = 10 * (0.99)**epoch # Adjust as needed\n#             if total_norm > max_norm:\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         param.grad *= max_norm / total_norm\n                        \n#             if batch_idx % 50 == 0: \n#                 total_norm = 0\n#                 for name, param in model.discriminator.named_parameters():\n#                     if param.grad is not None:\n#                         total_norm += torch.norm(param.grad).item() ** 2\n                        \n#                 total_norm = total_norm ** 0.5  # Take the square root to get the total norm\n#                 print('grad_norm:', total_norm)\n                \n            \n            imgs = imgs.to('cpu')\n        \n            \n            if batch_idx % 1000 == 0:\n\n                print(\"epoch:{}\".format(epoch))\n                print(\"-l_gan, l_class:\", -l_gan.item(), \" \", l_class.item())\n                print('Original Images')\n                imshow(torchvision.utils.make_grid(imgs[:10]))\n\n                # Pass images through the model to get the reconstructions\n                with torch.no_grad():\n\n                    imgs = imgs.to(device)\n                    mu, log_var = model.encoder(imgs)\n                    noise = torch.rand_like(log_var)\n                    z = mu + noise * torch.exp(0.5 * log_var)\n                    recon_images = model.decoder(z)\n                    recon_images = recon_images.cpu()\n                    imgs = imgs.to('cpu')\n\n                print('Reconstructed Images')\n                imshow(torchvision.utils.make_grid(recon_images[:10]))\n\n        if epoch%10 == 0:\n            torch.save(model.discriminator.state_dict(), 'disc_multi.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:23.365443Z","iopub.execute_input":"2024-06-17T17:08:23.365820Z","iopub.status.idle":"2024-06-17T17:08:23.384412Z","shell.execute_reply.started":"2024-06-17T17:08:23.365792Z","shell.execute_reply":"2024-06-17T17:08:23.383486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the models and optimizers\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = Encoder().to(device)\ndecoder = Decoder().to(device)\n# discriminator = Discriminator().to(device)","metadata":{"id":"j9P7UnA7z7l9","outputId":"42310e36-6714-4efe-8671-5a184f0a47d0","execution":{"iopub.status.busy":"2024-06-17T17:08:24.539321Z","iopub.execute_input":"2024-06-17T17:08:24.540038Z","iopub.status.idle":"2024-06-17T17:08:25.803870Z","shell.execute_reply.started":"2024-06-17T17:08:24.540007Z","shell.execute_reply":"2024-06-17T17:08:25.803026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.load_state_dict(torch.load('/kaggle/input/encoder_normalized/pytorch/v3/1/encoder.pt'))\ndecoder.load_state_dict(torch.load('/kaggle/input/decoder_normalized/pytorch/v3/1/decoder.pt'))\n# # discriminator.load_state_dict(torch.load('/kaggle/input/discriminator/pytorch/v1/1/disc.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:25.805317Z","iopub.execute_input":"2024-06-17T17:08:25.805576Z","iopub.status.idle":"2024-06-17T17:08:31.639245Z","shell.execute_reply.started":"2024-06-17T17:08:25.805554Z","shell.execute_reply":"2024-06-17T17:08:31.638370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vae_gan = VAE_GAN(encoder, decoder, discriminator).to(device)\noptimizer_enc = Adam(encoder.parameters(), lr = 3e-5)\noptimizer_dec = Adam(decoder.parameters(), lr = 3e-5)\n# optimizer_dis = Adam(discriminator.parameters(), lr=6e-5)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:31.641172Z","iopub.execute_input":"2024-06-17T17:08:31.641615Z","iopub.status.idle":"2024-06-17T17:08:31.646772Z","shell.execute_reply.started":"2024-06-17T17:08:31.641582Z","shell.execute_reply":"2024-06-17T17:08:31.645916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for imgs, _ in test_loader: \n    with torch.no_grad():\n\n        imgs = imgs.to(device)\n        mu, log_var = encoder(imgs)\n        noise = torch.rand_like(log_var)\n        z = mu + noise * torch.exp(0.5 * log_var)\n        recon_images = decoder(z)\n        recon_images = recon_images.cpu()\n        imgs = imgs.to('cpu')\n\n    print('Reconstructed Images')\n    imshow(torchvision.utils.make_grid(recon_images[:10]))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:08:31.647932Z","iopub.execute_input":"2024-06-17T17:08:31.648221Z","iopub.status.idle":"2024-06-17T17:08:40.542136Z","shell.execute_reply.started":"2024-06-17T17:08:31.648177Z","shell.execute_reply":"2024-06-17T17:08:40.540991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train(vae_gan, data_loader, optimizer_enc, optimizer_dec, optimizer_dis, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:54.753912Z","iopub.execute_input":"2024-06-15T08:11:54.754745Z","iopub.status.idle":"2024-06-15T08:11:54.758686Z","shell.execute_reply.started":"2024-06-15T08:11:54.754712Z","shell.execute_reply":"2024-06-15T08:11:54.757778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(vae_gan.discriminator.state_dict(), 'disc.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:54.914005Z","iopub.execute_input":"2024-06-15T08:11:54.914277Z","iopub.status.idle":"2024-06-15T08:11:54.918626Z","shell.execute_reply.started":"2024-06-15T08:11:54.914254Z","shell.execute_reply":"2024-06-15T08:11:54.917517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink \n# %cd /kaggle/working \n# FileLink('disc.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:55.175830Z","iopub.execute_input":"2024-06-15T08:11:55.176566Z","iopub.status.idle":"2024-06-15T08:11:55.180144Z","shell.execute_reply.started":"2024-06-15T08:11:55.176535Z","shell.execute_reply":"2024-06-15T08:11:55.179203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vae_gan.to('cpu')\n# vae_gan.eval()\n# out, _ = vae_gan.discriminator(train_dataset[8][0].unsqueeze(0))\n# out","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:55.336607Z","iopub.execute_input":"2024-06-15T08:11:55.336872Z","iopub.status.idle":"2024-06-15T08:11:55.340783Z","shell.execute_reply.started":"2024-06-15T08:11:55.336849Z","shell.execute_reply":"2024-06-15T08:11:55.339807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vals = []\n# vae_gan.to('cpu')\n# vae_gan.eval()\n# for el in tqdm(train_dataset):\n#     out, _ = vae_gan.discriminator(el[0].unsqueeze(0))\n#     vals.append(out.item())    ","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:55.619761Z","iopub.execute_input":"2024-06-15T08:11:55.620117Z","iopub.status.idle":"2024-06-15T08:11:55.624283Z","shell.execute_reply.started":"2024-06-15T08:11:55.620091Z","shell.execute_reply":"2024-06-15T08:11:55.623351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vals = np.array(vals)\n# np.mean(vals), np.std(vals)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:55.771714Z","iopub.execute_input":"2024-06-15T08:11:55.772162Z","iopub.status.idle":"2024-06-15T08:11:55.775855Z","shell.execute_reply.started":"2024-06-15T08:11:55.772135Z","shell.execute_reply":"2024-06-15T08:11:55.774919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoder = Encoder().to(device)\n# decoder = Decoder().to(device)\n# optimizer_enc = Adam(encoder.parameters(), lr=3e-5)\n# optimizer_dec = Adam(decoder.parameters(), lr=3e-5)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T08:11:55.940243Z","iopub.execute_input":"2024-06-15T08:11:55.940863Z","iopub.status.idle":"2024-06-15T08:11:55.944997Z","shell.execute_reply.started":"2024-06-15T08:11:55.940831Z","shell.execute_reply":"2024-06-15T08:11:55.943968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import DataLoader\n# from tqdm import tqdm\n\n# # Assuming `train_dataset` is a PyTorch dataset where each item is a tuple (image, label)\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Stack all images into a single tensor\n# all_images = torch.stack([train_dataset[i][0].squeeze().flatten() for i in range(len(train_dataset))]).to(device)\n# num_items = all_images.size(0)\n\n# closest_indices = []\n\n# # Compute pairwise distances using broadcasting\n# with torch.no_grad():\n#     # Compute the L2 distance between each pair of \"\"images\n#     distances = torch.cdist(all_images, all_images, p=2)  # Shape: (num_items, num_items)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:45:32.156691Z","iopub.execute_input":"2024-06-13T16:45:32.156999Z","iopub.status.idle":"2024-06-13T16:45:46.503206Z","shell.execute_reply.started":"2024-06-13T16:45:32.156970Z","shell.execute_reply":"2024-06-13T16:45:46.502164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with torch.no_grad():   \n#     for i in tqdm(range(num_items)):\n#         # Exclude the distance to itself by setting it to a high value\n#         distances[i, i] = float('inf')\n#         # Get the indices of the 10 smallest distances\n#         _, indices = torch.topk(distances[i], 10, largest=False)\n#         indices = indices.to('cpu')\n#         closest_indices.append(indices.cpu().tolist())\n\n# all_images = all_images.to('cpu')  # Move the tensor back to CPU if needed\n# distances = distances.to('cpu') ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:45:46.504496Z","iopub.execute_input":"2024-06-13T16:45:46.504865Z","iopub.status.idle":"2024-06-13T16:46:07.835485Z","shell.execute_reply.started":"2024-06-13T16:45:46.504816Z","shell.execute_reply":"2024-06-13T16:46:07.834691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# from scipy.linalg import lstsq\n\n# def approximate_target_matrix(target, matrices):\n#     # Flatten the target matrix\n#     target_flat = target.flatten()\n    \n#     # Stack flattened matrices as columns\n#     matrix_flat_stack = np.stack([matrix.flatten() for matrix in matrices], axis=1)\n    \n#     # Solve the least squares problem\n#     coefficients, _, _, _ = lstsq(matrix_flat_stack, target_flat)\n    \n#     return coefficients\n\n# coefficients = []\n# for i in tqdm(range(num_items)):\n    \n#     present = train_dataset[i][0].squeeze().numpy()\n#     indices = closest_indices[i] \n#     consts = []\n    \n#     for index in indices:\n#         consts.append(train_dataset[index][0].squeeze().numpy()) \n        \n#     coeffs = approximate_target_matrix(present, consts)\n#     coefficients.append(coeffs)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:46:07.838014Z","iopub.execute_input":"2024-06-13T16:46:07.838696Z","iopub.status.idle":"2024-06-13T16:49:01.580880Z","shell.execute_reply.started":"2024-06-13T16:46:07.838656Z","shell.execute_reply":"2024-06-13T16:49:01.579881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import numpy as np\n\n# class CustomImageDataset(Dataset):\n    \n#     def __init__(self, train_dataset, closest_indices, coefficients):\n#         \"\"\"\n#         Args:\n#             images (list of tensors): List of image tensors.\n#             closest_indices (list of lists): List where each element is a list of indices of the closest images.\n#             coefficients (list of lists): List where each element is a list of coefficients for the closest images.\n#         \"\"\"\n#         self.dataset = train_dataset\n#         self.closest_indices = closest_indices\n#         self.coefficients = coefficients\n\n#     def __len__(self):\n#         return len(train_dataset)\n\n#     def __getitem__(self, idx):\n        \n#         item = self.dataset[idx]\n#         closest_images = [self.dataset[i][0] for i in self.closest_indices[idx]]\n#         coeffs = self.coefficients[idx]\n\n#         closest_images_tensor = torch.stack(closest_images)\n#         coeffs_tensor = torch.tensor(coeffs, dtype=torch.float)\n        \n#         image = item[0]\n#         label = item[1]\n        \n#         return image, label, closest_images_tensor, coeffs_tensor\n\n# # Example usage\n# # if __name__ == \"__main__\":\n# #     # Dummy data\n# #     num_images = 100\n# #     image_shape = (3, 32, 32)\n# #     num_closest = 5\n\n# #     # Generate random images\n# #     images = [torch.rand(image_shape) for _ in range(num_images)]\n\n# #     # Generate random closest indices and coefficients\n# #     closest_indices = [np.random.choice(num_images, num_closest, replace=False).tolist() for _ in range(num_images)]\n# #     coefficients = [np.random.rand(num_closest).tolist() for _ in range(num_images)]\n\n# #     # Create the dataset\n# #     dataset = CustomImageDataset(images, closest_indices, coefficients)\n\n# #     # Create a DataLoader\n# #     dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# #     # Iterate over the DataLoader\n# #     for batch in dataloader:\n# #         images_batch, closest_images_batch, coeffs_batch = batch\n# #         print(\"Images batch shape:\", images_batch.shape)\n# #         print(\"Closest images batch shape:\", closest_images_batch.shape)\n# #         print(\"Coefficients batch shape:\", coeffs_batch.shape)\n# #         break\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.582167Z","iopub.execute_input":"2024-06-13T16:49:01.582512Z","iopub.status.idle":"2024-06-13T16:49:01.592944Z","shell.execute_reply.started":"2024-06-13T16:49:01.582477Z","shell.execute_reply":"2024-06-13T16:49:01.592145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new_dataset = CustomImageDataset(train_dataset, closest_indices, coefficients)\n# new_dataloader = DataLoader(new_dataset, shuffle = True, batch_size = 24)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.594193Z","iopub.execute_input":"2024-06-13T16:49:01.594472Z","iopub.status.idle":"2024-06-13T16:49:01.607775Z","shell.execute_reply.started":"2024-06-13T16:49:01.594450Z","shell.execute_reply":"2024-06-13T16:49:01.606880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.608849Z","iopub.execute_input":"2024-06-13T16:49:01.609113Z","iopub.status.idle":"2024-06-13T16:49:01.777623Z","shell.execute_reply.started":"2024-06-13T16:49:01.609064Z","shell.execute_reply":"2024-06-13T16:49:01.776696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import gc\n\n# def get_cuda_tensors():\n#     cuda_tensors = []\n#     for obj in gc.get_objects():\n#         try:\n#             if torch.is_tensor(obj) and obj.is_cuda:\n#                 cuda_tensors.append(obj)\n#         except:\n#             pass\n#     return cuda_tensors\n\n# # Usage example\n# cuda_tensors = get_cuda_tensors()\n# for tensor in cuda_tensors:\n#     print(tensor.size(), tensor.device)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:49:01.779165Z","iopub.execute_input":"2024-06-13T16:49:01.779606Z","iopub.status.idle":"2024-06-13T16:49:01.990351Z","shell.execute_reply.started":"2024-06-13T16:49:01.779571Z","shell.execute_reply":"2024-06-13T16:49:01.989449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 30\n# for epoch in range(epochs):\n    \n#     total_loss = 0\n#     count = 0\n#     for image, _ , closest_images_tensor, coeffs_tensor in new_dataloader:\n        \n#         optimizer_enc.zero_grad()\n#         optimizer_dec.zero_grad()\n        \n#         closest_images_tensor = closest_images_tensor.to(device)\n#         coeffs_tensor = coeffs_tensor.to(device)\n#         image = image.to(device)\n        \n#         mu, log_var = encoder(image)\n#         noise = torch.rand_like(log_var)\n#         my_z = mu + noise * torch.exp(0.5 * log_var) #[24, 10]\n#         recons = decoder(my_z)\n        \n#         criterion = nn.MSELoss(reduction = 'sum')\n        \n#         # closest_images_tensor of shape [24, 10, 1, 28, 28]\n#         batch_size = closest_images_tensor.shape[0]\n#         dost_sums = []\n#         for i in range(batch_size):\n#             sub_batch = closest_images_tensor[i, :, : , : , :] # [1, 10, 1, 28, 28]\n#             sub_batch = sub_batch.squeeze(0) # [10, 1, 28, 28] \n#             mu, log_var = encoder(sub_batch) \n#             noise = torch.rand_like(log_var)\n#             z = mu + noise * torch.exp(0.5 * log_var) # [128, 10]\n#             dost_recons_sum = z.T @ coeffs_tensor[i] # [128, 1]\n#             dost_sums.append(dost_recons_sum.unsqueeze(1).T) \n        \n#         dost_recons_sum = torch.stack(dost_sums).squeeze() #[batch_size, 128] \n        \n#         loss1 = criterion(image, recons)\n#         loss2 = torch.mean(torch.sum((my_z - dost_recons_sum)**2, axis = 1))\n#         loss = 0.1*loss1 + loss2 - 5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n        \n#         loss.backward()\n#         optimizer_enc.step()\n#         optimizer_dec.step()\n#         total_loss += loss\n#         count +=1\n        \n#         closest_images_tensor = closest_images_tensor.to('cpu')\n#         coeffs_tensor = coeffs_tensor.to('cpu')\n#         image = image.to('cpu')\n        \n#     print('Original Images')\n#     imshow(torchvision.utils.make_grid(image[:10]))\n\n#     # Pass images through the model to get the reconstructions\n#     with torch.no_grad():\n\n#         image = image.to(device)\n#         mu, log_var = encoder(image)\n#         noise = torch.rand_like(log_var)\n#         z = mu + noise * torch.exp(0.5 * log_var)\n#         recon_images = decoder(z)\n#         recon_images = recon_images.cpu()\n#         image = image.to(device)\n\n#     print('Reconstructed Images')\n#     imshow(torchvision.utils.make_grid(recon_images[:10]))\n#     print(\"The epoch is {} and the loss is {}\".format(epoch, total_loss/count))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T17:42:02.919661Z","iopub.execute_input":"2024-06-13T17:42:02.920443Z","iopub.status.idle":"2024-06-13T17:51:48.028878Z","shell.execute_reply.started":"2024-06-13T17:42:02.920409Z","shell.execute_reply":"2024-06-13T17:51:48.027378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.save(encoder.state_dict(), 'encoder.pt')\n# torch.save(decoder.state_dict(), 'decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:48:44.564411Z","iopub.execute_input":"2024-06-09T10:48:44.564671Z","iopub.status.idle":"2024-06-09T10:48:44.577123Z","shell.execute_reply.started":"2024-06-09T10:48:44.564643Z","shell.execute_reply":"2024-06-09T10:48:44.576397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# %cd /kaggle/working\n# FileLink('encoder.pt')3","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:48:44.578207Z","iopub.execute_input":"2024-06-09T10:48:44.578778Z","iopub.status.idle":"2024-06-09T10:48:44.587703Z","shell.execute_reply.started":"2024-06-09T10:48:44.578747Z","shell.execute_reply":"2024-06-09T10:48:44.586944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# %cd /kaggle/working\n# FileLink('decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-09T10:48:44.588738Z","iopub.execute_input":"2024-06-09T10:48:44.589544Z","iopub.status.idle":"2024-06-09T10:48:44.597115Z","shell.execute_reply.started":"2024-06-09T10:48:44.589513Z","shell.execute_reply":"2024-06-09T10:48:44.596432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_multi = Discriminator_multi().to(device)\nvae_gan_multi = VAE_GAN_multi(encoder, decoder, discriminator_multi).to(device)\noptimizer_dis = Adam(discriminator_multi.parameters(), lr = 3e-5)","metadata":{"id":"iHqyUNNhY_CX","execution":{"iopub.status.busy":"2024-06-17T17:08:53.828003Z","iopub.execute_input":"2024-06-17T17:08:53.828383Z","iopub.status.idle":"2024-06-17T17:08:54.372064Z","shell.execute_reply.started":"2024-06-17T17:08:53.828354Z","shell.execute_reply":"2024-06-17T17:08:54.371321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_multi.load_state_dict(torch.load('/kaggle/input/discriminator_multi_normalized/pytorch/v2/1/disc_multi.pt'))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:09:02.924695Z","iopub.execute_input":"2024-06-17T17:09:02.925098Z","iopub.status.idle":"2024-06-17T17:09:06.275804Z","shell.execute_reply.started":"2024-06-17T17:09:02.925069Z","shell.execute_reply":"2024-06-17T17:09:06.275003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def clip_gradients(grad, clip_value=0.5):\n#     return grad.clamp_(-clip_value, clip_value)\n\n# # Attach gradient clipping hook to each parameter\n# for param in vae_gan_multi.discriminator.parameters():\n#     if param.requires_grad:\n#         param.register_hook(lambda grad: clip_gradients(grad))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T17:17:30.458840Z","iopub.execute_input":"2024-06-03T17:17:30.459209Z","iopub.status.idle":"2024-06-03T17:17:30.464928Z","shell.execute_reply.started":"2024-06-03T17:17:30.459179Z","shell.execute_reply":"2024-06-03T17:17:30.463953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.encoder.eval()\nvae_gan_multi.decoder.eval()\nvae_gan_multi.discriminator.train()\ntrain_multi(vae_gan_multi, data_loader, optimizer_dis, None, None,  epochs = 100)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T17:09:17.174490Z","iopub.execute_input":"2024-06-17T17:09:17.174852Z","iopub.status.idle":"2024-06-17T17:11:25.640814Z","shell.execute_reply.started":"2024-06-17T17:09:17.174810Z","shell.execute_reply":"2024-06-17T17:11:25.639577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(vae_gan_multi.discriminator.state_dict(), 'disc_multi.pt')\n# torch.save(vae_gan_multi.encoder.state_dict(), 'encoder.pt')\n# torch.save(vae_gan_multi.decoder.state_dict(), 'decoder.pt')","metadata":{"execution":{"iopub.status.busy":"2024-06-02T08:37:48.361662Z","iopub.execute_input":"2024-06-02T08:37:48.362480Z","iopub.status.idle":"2024-06-02T08:37:49.685660Z","shell.execute_reply.started":"2024-06-02T08:37:48.362451Z","shell.execute_reply":"2024-06-02T08:37:49.684544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vae_gan_multi.eval()\n# vae_gan_multi.discriminator.train()\n# vae_gan_multi.to('cpu')\nfor imgs, labels in test_loader:\n    imgs = imgs.to(device)\n    features_real, features_recon, mu, log_var, disc_real, disc_recon, disc_prior, class_real = vae_gan_multi(imgs)\n    print(torch.mean(disc_real), torch.mean(disc_recon), torch.mean(disc_prior))\n    imgs = imgs.to('cpu')\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vals = []\n# vae_gan_multi.to('cpu')\n# vae_gan_multi.eval()\n# for i in tqdm(range(len(train_dataset))):\n    \n#     x = train_dataset[i][0].unsqueeze(0).to(device)\n#     out, _, _ = vae_gan_multi.discriminator(x)\n#     vals.append(out.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vals = np.array(vals)\n# np.mean(vals), np.std(vals)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T18:26:26.499379Z","iopub.execute_input":"2024-05-17T18:26:26.499692Z","iopub.status.idle":"2024-05-17T18:26:26.506360Z","shell.execute_reply.started":"2024-05-17T18:26:26.499666Z","shell.execute_reply":"2024-05-17T18:26:26.505474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}